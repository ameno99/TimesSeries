---
title: "TIMES SERIES"
author: "Cheikh Mbacké BEYE"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message=FALSE,
                      comment = NA,
                      warning = NA,
                      fig.width=12,
                      fig.height = 6
                      )
```
<style>
div.blue pre {background-color:#ccffcc;}
div.blue pre.r{background-color:#ccffff;}

</style>
<div class='blue'>
```{r}
library(ggplot2)
library(astsa)
library(xts)
```



# INTRODUCTION

Data obtained from observations collected sequentially over time are extremely common. In bussines we observe weekly interest rates, daily closing stock prices, monthly prices indices, yearly sales figures, and so forth. In meterology, we observe daily  temperatures, annual precipitation and drought indices and hourly wind speeds. In agriculture, we record annual figures for crop and livestock prooduction, soil erosion, and export sales. In the biological science , we observe the electrical activity of the heart at millisecond intervals. In ecology, we record the abundance of animal species. The list of areas in which **times series** are studies is virtually endless. The purpose of time series analysis is generaly twofold : to undestand or model the stochastic mechanism that gives rise to an observed series and to predict or forecast the future values of a series based on the history of that series and possibily other related factors.
We will introduces a variety of examples of time series from diverse areas of application. A somewhat unique feature of time series and their models is that we usually cannot assume that the observations are independently from a common population. Studying models that incoperate dependence is the key concept in time series analysis.

# TIME SERIES EXAMPLES

## Johnson and Johnson Quarterly Earnings Per Share


```{r}
jj
tseries<-data(jj)
plot(jj,
     type='o',
     ylab='Quartely Earnings per Share',
     frame=FALSE,
     col='steel blue'
     )
```

This figure shows quaterly earnings per shape for the US company Johnson and Johnson, furnished by Professor Paul Griffin. There are 84 quaters (84/4=21 years) measured from the first quarter of 1960 to the last quarter of 1980. Modelling such a series begins by observing the primary patterns in the time history?. In this case, note the gradualy increasing underlying trend anf the rather regular variation superimposedon on the trend that seems to repeat over quarters .

## Global mean land-ocean temperature deviations to 2015
```{r}
globtemp
tseries<-data(jj)
plot(globtemp,
     type='o',
     ylab='Global temperture deviations over year',
     frame=FALSE,
     col='steel blue'
     )
```
The figure shows the global temperature series record. The data are the global mean land-ocean temperature index from 1880 to 2015. We note an apparent upward trend in the series during the latter part of the twentieth century that has been used as an argument for the gloabal warming hypothesis. Note also the leveling off at about 1935 and then another rather sharp upward trend at about 1970.
## Dow Jones Industrial Average

```{r}
djia[1:10]
djiaReturns<-diff(log(djia$Close))[-1]
plot(djiaReturns,
     main='DJIA returns',
     frame=FALSE,
     col='steel blue'
     )
```
This an example of financial time series data. It shows the daily returns of the Dow Jones Industrial Average from april 20, 2006 to april 20, 2016. This a typical of  return data.  The mean of the series appears to be stable with an average return approximately zero. A problem in the nalaysis of these type of financial data is to forcast the volatility of future returns. It's easy to spot the financial crisis of 2008.

## Seismic Trace of Explosion and  Earthquake  


```{r}
plot(EXP6,
     main='Expolsion',
     frame=FALSE,
     col='steel blue'
     )
plot(EQ5,
     main='Earthquake',
     frame=FALSE,
     col='steel blue'
)
```
These two last examples represent two phases denoted by $P(t=1,...;1024)$ and $S(t=1025,...,2048)$ at a seismic recording station. The recording instruments in Scandinavia are observing earthquake and mining explosion. The general problem of intrest is in distinguishing or discriminating between waveforms generated by earthquakes and those generated by explosion . We can also focus oçn the amplitude ratios between the two phases,which tend to be smaller for earhtquakes than for explosions.

# WHITE NOISE

A simple kind of generated series might be a collection of uncorrelated random variables, $(\epsilon_t)_{t\in \mathbb{Z}}$ with mean 0 and finite variance $\sigma^2$. The time series generated from uncorrelated variables is used as a model for noise in engenerring applications where it is called $white~noise$. The designation white originates from the analogy with white light and indicates that all possible periodic oscilllations are present with equal strength.

We will sometimes require the noise to be independent and identically distributed (iid). A particulary useful white noise series is Gaussian white noise  where $\epsilon_t\approx\mathcal{N}(0,\sigma^2)$

```{r}
set.seed(123)
plot.ts(rnorm(500),
        frame=FALSE,
        main='White noise',
        col='steel blue'
        )
```

We note mixture of many diffirent kinds of oscillations. But the white noise alone cannot explained all time series behavior. If it was the case, classical statistical methods would suffice. 
To model a time series for forcasting or predicting purpose, we should take account of serial correlation between observations.

# MOVING AVERAGE

We might replace the white noise series $\epsilon_t$ by a moving average that smooths the series. For example, consider replacing $\epsilon_t$ by an average of its current value and its immediate neighbors in the past and future. That is, let

$$X_t=\frac{1}{3}\big(\epsilon_{t-1}+\epsilon_{t}+\epsilon_{t-2}\big)$$
To get the moving averge we can use the command $filter$ in $stats$ package.

filter($x$, $filter$, $method$= c("convolution", "recursive"),$sides$= 2, $circular$ = FALSE)

$\textbf{Arguments}$:\

 * x	a univariate or multivariate time series.

* filter a vector of filter coefficients in reverse time order (as for AR or MA coefficients).

* method	Either $convolution$ or $recursive$ (and can be abbreviated). If "convolution" a moving average is used: if "recursive" an autoregression is used.

* sides	for convolution filters only. If sides = 1 the filter coefficients are for past values only; if sides = 2 they are centred around lag 0. In this case the length of the filter should be odd, but if it is even, more of the filter is forward in time than backward.

* circular for convolution filters only. If TRUE, wrap the filter around the ends of the series, otherwise assume external values are missing (NA).

```{r}
ma<-stats::filter(rnorm(500),
                  sides = 2,
                  method = 'convolution',
                  filter=rep(1/3,3),
                )
plot.ts(ma,
     main = 'Moving averge',
     frame.plot = FALSE,
     col='steel blue'
     )
```

Oscillations  are more apparent and some of the faster oscillations are taken out.

# AUTOREGRESSIONS

$$X_t=X_{t-1}-9X_{t-2}+\epsilon_t$$
This equation represnts a regression or prediction of the current value $X_{t}$ of a times series as function of the past two values of the series. A problem with startup values exists because the equation depends on the initial conditions.

The function $filter$ uses  zero for the initial values. To fix that we can run $filter$ for more than needed and remove the initial valaues.

```{r}

areg<-stats::filter(rnorm(550),
                  method = 'recursive',
                  filter=c(1,-.9)
                )
plot.ts(areg[-(1:50)],
     main = 'Autoregression',
     frame.plot = FALSE,
     col='steel blue'
     )
```


# RANDOM WALK WITH DRIFT

A model for anlysing trend such as seen in the global temperature data is the random walk with drift model given by 
$$X_t=\delta+X_{t-1}+\epsilon_t ;\quad X_0=0$$
$\delta$ is called drift and when $\delta=0$ is called simply a random walk.

$$X_t=\delta t+\sum_{i=1}^{t}\epsilon_i$$

```{r}
wn<-rnorm(200)
rwd<-cumsum(wn+0.2)
rw<-cumsum(wn)
plot.ts(rwd,
        main='Random walk with drift',
        col='steel blue',
        frame=FALSE,
        ylim=c(-5,50)
        )
lines(rw,
      col='blue'
      )
abline(h=0,
       lty=2,
       col='blue'
       )
abline(a=0,
       b=0.2,
       lty=2,
       col=' steel blue'
       )


```

# SIGNAL IN NOISE
Many realistic models for generating time series assume an undeerlying signal with some consitent periodic variation , contaminated by adding a random noise. 

$$X_t=2\cos\big(2\pi\frac{t+15}{50}\big)+\epsilon_t$$


* A=2 is the amplitude

* $\omega=\frac{1}{50}$ is the frequency of oscillations (one cycle every 50 time points)

* $\phi=2\pi\frac{15}{50}=0.6\pi$ is the phase shift
```{r}
wn<-rnorm(500)
cosinus<-2*cos(2*pi*1:500/50+.6*pi)
plot.ts(cosinus,
        col='steel blue',
        frame=FALSE,
        main=expression(2*cos(2*pi*t/50+.6*pi))
        )
plot.ts(cosinus+wn,
        col='steel blue',
        frame=FALSE,
        main=expression(2*cos(2*pi*t/50+.6*pi)+N(0,1))
        )
plot.ts(cosinus+wn,
        col='steel blue',
        frame=FALSE,
        main=expression(2*cos(2*pi*t/50+.6*pi)+N(0,20))
        )
            
```


# MEAN FUNCTION 

The mean fucntion is defiined as

$$\mu_t=\int_{-\infty}^{+\infty} x X_t\mathrm{dx}$$

Example the mean function of the moving average seen in the preview chapter is 
$$\mu_t=\mathbb{E}(\frac{1}{3}\big[\epsilon_{t-1}+\epsilon_{t}+\epsilon_{t-2}\big])$$
$$\mu_t=0$$

$\textbf{Random walk}:$\

$$\mu_t=\mathbb{E}\big(\delta t+\sum_{i=1}^{t}\epsilon_i\big)$$
$$\mu_t=\delta t$$
The mean fucntion of a random walk is a stright line with a slope equal to the random walk's drift $\delta$.


# AUTOCOVARIANCE FUNCTION 

The auocovariance function is defined as the second moment product 

$$\gamma(s,t)=cov(X_t,X_s)=\mathbb{E}\big[(X_t-\mu_t)(X_s-\mu_s)\big]$$
The auocovariance mesures the linear dependence between two points of the same series observed at the different times. Very smooth series exhibit autocovariance functions that stay large even when the t and s are far apart, whereas choppy series trend to have autocovariance tend to have a autocovariance functions that are nearly zero for large seperations.\
If $\gamma(s,t)=0$,  there is no linear dependence between $X_t$ and $X_s$ but there still may be some dependence strucuture. However if  $X_t$ and $X_s$ are bivariate normal, $\gamma(s,t)=0$ ensures their independences.
$$var(X_t)=\mathbb{E}\big[(X_t-\mu_t)^2\big]=\gamma(t)$$

# CLASSICAL REGRESSION IN TIMES SERIES CONTEXT

We begin our discusssion of linear regression in the time series context by assiming some ouput or dependaent time series say , $X_t$ for $t=1,..., t=n$ is being influenced by a collection of possible inputs or independent series say, $Z_{t_1},...,Z_{t_q}$, where we first regard the inputs as fixed and known. This assumption is for applying conventional linear regression.
We express this relationship through the model

$$X_t=\beta_0+\beta_1Z_{t_1}+...+\beta_{t_q}Z_{t_q}+\epsilon$$
Where the paremeters $\beta_0, \beta_1,...,\beta_q$ are unknown and $(\epsilon_t)$ is random error or noise process consisting of indepndent and identically distrinuted normal variables.

## Example

```{r,fig.height=12}
fit<-lm(AirPassengers~time(AirPassengers),na.action = NULL)
summary(fit)
par(mfrow=c(2,1))
plot(AirPassengers,col='steel blue',main='Air passengers')
abline(fit,col='red',lwd=2)
hist(fit$residuals,col = 'steelblue', border = 'cyan')
```

